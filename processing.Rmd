---
title: "Standardization, normalization and ?-diversity index"
author: "Casper Sahl Poulsen"
date: "30102018"
output:
  html_document:
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE, 
                      message = FALSE,
                      fig.align = "center")
```

## Introduction
The purpose of the following code is to assess the effect of standardization, normalization and distance metrics used for compositional ecology data. This is typical for metagenomics or 16S data used for taxonomic characterisation of microbial communities.  

### Assumptions
**Test data** is generated to assess classical challanges with this type of data. This include randomness, sequencing depth, sparsity and structural properties. The data was constructed to reflect the challanges by simulating the sample structure as follow:
**Organisms in reference**
*Organism1* highly abundant random count between 1000-5000
*Organism2-4* medium high abundance three random counts between 100:999
*Organism5-13* medium abundance 9 random counts between 5:99
*Organism14-40* low abundance 27 random counts between 0:4
*Organism41-70* Not present
**Sample**  
*S1Ref:* Reference  
*S2RanRepRef:* Random replicate  
*S3ExSF2:* Exact scaling factor 2  
*S4ExSF10:* Exact scaling factor 10  
*S5RanSF2:* Random scaling factor 2  
*S6RanSF10:* Random scaling factor 10  
*S7RanSwHaMHa:* Switching a highly abundant and one of the organisms with medium high abundance generated random  
*S8RanSwHaMa:* Switching a highly abundant and one of the organisms with medium abundance generated random  
*S9RanSwHaLa:* Switching a highly abundant and one of the organisms with low abundance generated random  
*S10RanSwHaNP:* Switching a highly abundant and one of the organisms with one that is not present random  
*S11ExSwHaMHa:* Switching a highly abundant and one of the organisms with medium high abundance generated exact  
*S12ExSwHaMa:* Switching a highly abundant and one of the organisms with medium abundance generated exact  
*S13ExSwHaLa:* Switching a highly abundant and one of the organisms with low abundance generated exact  
*S14ExSwHaNP:* Switching a highly abundant and one of the organisms with one that is not present exact  
*S15RanizeRefV:* Randomize Sample1 vector  
*S16ExRevRef:* Reverse Sample1 vector exact  
*S17RanRevRef:* Reverse Sample1 vector random  
*S18ExSwHato0:* Change high abundance to 0 exact  
*S19ExSwMHato0:* Change all medium high abundance to 0 exact  
*S20ExSwMato0:* Change all medium abundance to 0 exact  
*S21ExSwLato0:* Change all low abundance to 0 exact  
  
Testdata were constructed to be simple and represent easy to follow calculations for the reader. As a consequence testdata is not highly multivariate another common challenge in this type of data. 


**Evaluation** of test data are represented in tables comparing the reference(Sample1) with the other samples.     


### Disclaimer  
It is important to be aware that the pipelines does not provide a single solution to a multifactorial problem and the appropriate decision regarding the applicabilty of a pipeline is inherently depending on the data at hand. 

### Packages 
```{r}
library(vegan) #Community ecology package, ordination methods, diversity analysis and other functions for community and vegetation ecologists. help(package="vegan")
library(rlang)
library(Rcpp)
library(tidyselect)
library(dplyr) #A grammar of data manipulation. A fast, consistent tool for working with data frame like objects. help(package="dplyr")
library(knitr) #A General-Purpose Package for Dynamic Report Generation in R help(package="knitr")
library(ggplot2)
library(pheatmap) # Implementation of heatmaps that offers more control over dimensions and appearance help(package="pheatmap")
library(zCompositions)
library(compositions)
library(reshape)
library(tidyr)
library(gridExtra)
library(cowplot)
library(stringi)
library(ggthemes) #This package contains extra themes, scales, and geoms, and functions for and related to ggplot2 eg. scale_colour_gdocs() help(package="ggthemes")
library(scales)
library(RColorBrewer)

#source("http://bioconductor.org/biocLite.R")
#biocLite("metagenomeSeq")
library(metagenomeSeq)
library(digest)
library(DESeq2)
library(edgeR)
library(plotly)

#source("http://bioconductor.org/biocLite.R")
#biocLite("ggtree")
library(ggtree)
library(harrietr)
library(stringr)
```

## Analysis
### Create count table (Testdata) and metadata (Metadata)
```{r}
#Naming Ref=Reference, Ran=Random, Ex=Exact, Rep=Replicate, SF=Scaling Factor, Rev=Reverse, Sw=Switching, Ha=High abundance,  MHa=Medium high abundance, Ma=Medium abundance, La=Low abundance, NP=Not present, V=Vector, Rev=Reverse
set.seed(1)
S1Ref<-c(sample(1000:5000, 1, replace=TRUE), sample(100:999, 3, replace=TRUE), sample(5:99, 9, replace=TRUE), sample(0:4, 27, replace=TRUE), rep(0, times=30))
set.seed(2)
Testdata <- data.frame(S1Ref=S1Ref, 
                       S2RanRepRef=c(sample(1000:5000, 1, replace=TRUE), sample(100:999, 3, replace=TRUE), sample(5:99, 9, replace=TRUE), sample(0:4, 27, replace=TRUE), rep(0, times=30)), 
                       S3ExSF2=S1Ref*2,
                       S4ExSF10=S1Ref*10,
                       S5RanSF2=c(sample(2000:10000, 1, replace=TRUE), sample(200:1999, 3, replace=TRUE), sample(10:199, 9, replace=TRUE), sample(0:9, 27, replace=TRUE), rep(0, times=30)),
                       S6RanSF10=c(sample(10000:50000, 1, replace=TRUE), sample(1000:9999, 3, replace=TRUE), sample(50:999, 9, replace=TRUE), sample(0:49, 27, replace=TRUE), rep(0, times=30)),
                       S7RanSwHaMHa=c(sample(100:999, 3, replace=TRUE), sample(1000:5000, 1, replace=TRUE), sample(5:99, 9, replace=TRUE), sample(0:4, 27, replace=TRUE), rep(0, times=30)),
                       S8RanSwHaMa=c(sample(5:99, 1, replace=TRUE), sample(100:999, 3, replace=TRUE), sample(5:99, 8, replace=TRUE), sample(1000:5000, 1, replace=TRUE), sample(0:4, 27, replace=TRUE), rep(0, times=30)),
                       S9RanSwHaLa=c(sample(0:4, 1, replace=TRUE), sample(100:999, 3, replace=TRUE), sample(5:99, 9, replace=TRUE), sample(0:4, 26, replace=TRUE), sample(1000:5000, 1, replace=TRUE), rep(0, times=30)),
                       S10RanSwHaNP=c(0, sample(100:999, 3, replace=TRUE), sample(5:99, 9, replace=TRUE), sample(0:4, 27, replace=TRUE), rep(0, times=29), sample(1000:5000, 1, replace=TRUE)),
                       S11ExSwHaMHa=c(S1Ref[4],S1Ref[2:3],S1Ref[1], S1Ref[5:length(S1Ref)]),
                       S12ExSwHaMa=c(S1Ref[13],S1Ref[2:12],S1Ref[1], S1Ref[14:length(S1Ref)]),
                       S13ExSwHaLa=c(S1Ref[40],S1Ref[2:39],S1Ref[1], S1Ref[41:length(S1Ref)]),
                       S14ExSwHaNP=c(S1Ref[70],S1Ref[2:69],S1Ref[1]),
                       S15RanizeRefV=c(sample(S1Ref, replace=FALSE)),
                       S16ExRevRef=rev(S1Ref), #If you hash this line out reverse exact is removed also have to change below in generation of metadata
                       S17RanRevRef=rev(c(sample(1000:5000, 1, replace=TRUE), sample(100:999, 3, replace=TRUE), sample(5:99, 9, replace=TRUE), sample(0:4, 27, replace=TRUE), rep(0, times=30))),
                       S18ExSwHato0=c(0, S1Ref[2:length(S1Ref)]),
                       S19ExSwMHato0=c(S1Ref[1], rep(0, times=3), S1Ref[5:length(S1Ref)]),
                       S20ExSwMato0=c(S1Ref[1:4], rep(0, times=9), S1Ref[14:length(S1Ref)]),
                       S21ExSwLato0=c(S1Ref[1:13], rep(0, times=27), S1Ref[41:length(S1Ref)])
                       )
row.names(Testdata) <- c(paste("Organism", 1:70, sep=""))

#Create metadata
Metadata<-data.frame(Sample=colnames(Testdata)) #Can add if needed, row.names=colnames(Testdata)

Metadata$Type <-
          ifelse(seq(along=(Metadata$Sample)) %in% grep("*Ran*", Metadata$Sample), "Random",
          ifelse(seq(along=(Metadata$Sample)) %in% grep("*Ex*", Metadata$Sample), "Exact",
          ifelse(seq(along=(Metadata$Sample)) %in% grep("*Ref*", Metadata$Sample), "Ref",
                 "Other")))
Metadata$Test <- c(rep("Ref", time=2), rep("SeqDepth", time=4), rep("Switch", time=8), "Randomize", rep("Reverse", time=2), rep("Zero", time=4))
#Metadata$Test <- c(rep("Ref", time=2), rep("SeqDepth", time=4), rep("Switch", time=8), "Randomize", rep("Reverse", time=1), rep("Zero", time=4)) # if removing reverse exact

Metadata$Level <- c(rep("Nothing", time=2), rep(c("Low", "High"), time=2), rep(c("MediumHigh", "Medium", "Low", "NotPresent"), time=2), rep("Nothing", time=3), "High", "MediumHigh", "Medium", "Low")
#Metadata$Level <- c(rep("Nothing", time=2), rep(c("Low", "High"), time=2), rep(c("MediumHigh", "Medium", "Low", "NotPresent"), time=2), rep("Nothing", time=2), "High", "MediumHigh", "Medium", "Low") # if removing reverse exact

rm(S1Ref)


```


## Define Color and shape scheme
```{r}
#Define coloring schemes
TestCol<-c("Randomize" = "#FFFF33", "Ref" = "#377EB8", "Reverse" = "#E41A1C", "SeqDepth" = "#4DAF4A", "Switch" = "#984EA3", "Zero" = "#FF7F00") #To create PCoA/PCA
ProCCol<-c("chisq" = "#E41A1C", "freq" = "#4DAF4A", "max" = "#FFFF33", "norm" = "#A65628", "clr"="#E41A1C", "ilr"="#9f1214", "CSS"="#ed5e5f", "DESeq"="#377EB8", "TMM"="#265880", "hellinger"="#4DAF4A", "TSS_hellinger"="#357933", "log"="#984EA3", "TSS_log"="#5b2e61", "pa"="#FF7F00", "Rarefy"="#FFFF33", "TSS"="#A65628") #To create procrustes PCoA/PCA
           
##Define shape schemes
#table(Metadata$Level)
LevelShape<-c("Nothing"=1, "High"=0, "MediumHigh"=2, "Medium"=6, "Low"=5, "NotPresent"=8) #To create PCoA/PCA
ProCShape<-c("altGower"=9, "binomial"=5, "bray"=3, "canberra"=6, "euclidean"=0, "gower"=8, "horn"=2, "jaccard"=1, "kulczynski"=7, "manhattan"=4) #To create procrustes PCoA/PCA
annotation_colorsHeat = list(betadiversity = c(bray = "#E41A1C", euclidean= "#377EB8", manhattan="#4DAF4A"), Preprocessing = c(clr="#E41A1C", ilr="#9f1214", CSS="#ed5e5f", DESeq="#377EB8", TMM="#265880", hellinger="#4DAF4A", hellingerTSS="#357933", log="#984EA3", logTSS="#5b2e61", pa="#FF7F00", Rar="#FFFF33", TSS="#A65628")) #To create 
```


### Standardization and dissimilarity indices compared
Creating list of all dissimilarity objects (DistList) 
Creating df with dissimilarities relative to the reference using real values (df) and ranks (dfrank)  
The same calculations were made subsetting only to exact modifications of the ref, both as real values and ranks 
```{r}
###############################Classical##################################
##Methods provided in vegan
#All vegdist: "manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "morisita", "horn", "mountford", "raup" , "binomial", "chao", "cao", "mahalanobis"
#All decostand: "total", "max", "freq", "normalize", "range", "standardize", "pa", "chi.square", "hellinger", "log"
#wisonsin call alone 
#Combinations canberra+range, bray+range, bray+standardize, kulczynski+range, produces error removed range and standardize
#Counts work with chao, cao, mahalanobis removed them as well
#Works with combining
#"manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial"
#"total", "max", "freq", "normalize", "pa", "hellinger", "log"

#If TSS is performed before other transformations/standardizations/normalizations
df<-data.frame()
DistList <- list()
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  for (j in c("total", "max", "freq", "normalize", "pa", "hellinger", "log")) {
    distmatrix<-vegdist(decostand(decostand(t(Testdata), method="total"), method=j), method=i)
    #Create list containing the dist matrices
    distName <- paste( 'dist', i, j, sep = '' )
    DistList[[ distName ]]<-distmatrix
    #Make distances into matrix
    distmatrix <- data.frame(as.matrix(distmatrix))
    deco<-j
    dissi<-i
    df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
  }
}

#If TSS is performed after other transformations/standardizations/normalizations
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  for (j in c("total", "max", "freq", "normalize", "pa", "hellinger", "log")) {
    distmatrix<-vegdist(decostand(decostand(t(Testdata), method=j), method="total"), method=i)
    #Create list containing the dist matrices
    distName <- paste( 'dist', i, "total", j, sep = '' )
    DistList[[ distName ]]<-distmatrix
    #Make distances into matrix
    distmatrix <- data.frame(as.matrix(distmatrix))
    deco<-paste(j, "TSS", sep="")
    dissi<-i
    df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
  }
}

##Rarefying using the rrarefy function from vegan
dataRar <- t(rrarefy(t(Testdata), min(colSums(Testdata))))
j <- "Rar"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataRar), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}

##CSS method from metagenomeseq. The setup to run metagenomeseq is from MixMC
data.metagenomeSeq <- newMRexperiment(Testdata, featureData=NULL, libSize=NULL, normFactors=NULL)  
p <- cumNormStat(data.metagenomeSeq) #default is 0.5
data.cumnorm <- cumNorm(data.metagenomeSeq, p=p)
#CSS data
dataCSS <- MRcounts(data.cumnorm, norm=TRUE, log=TRUE) #A log transformation is also performed as recommended in MixMC 
j <- "CSS"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataCSS), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}


## Methods from DESeq2
#Create design formula, not important which column is used in metadata since no DA statistics are obtained, but only used to extract size factors.
#The type="ratio" can not be used since all features have at least one zero instead using the "poscounts". 
design <- formula(paste("~ ", "Level"))
#Create DESeq2 object with matrix and metadata
dds <- DESeqDataSetFromMatrix(countData = Testdata,
                              colData = Metadata,
                              design = design)
dds <- estimateSizeFactors(dds, type="poscounts")
dataDESeq<-sweep(Testdata, MARGIN = 2, sizeFactors(dds), FUN = "/")
j <- "DESeq"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataDESeq), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}

##TMM method from edgeR. Both normalizing according to y$samples$lib.size and y$samples$norm.factors 
#DGEList, not important which column is used in metadata since no DA statistics are obtained.
y <- DGEList(counts=Testdata,group=Metadata$Type)
y <- calcNormFactors(y, method="TMM")
dataTMM<-cpm(y)
j <- "TMM"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataTMM), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}


###############################Coda framework##################################
##Performing zero estimation before total sum scaling, have some implications in that library size with exact multiplication suddenly is not the same after TSS. 
#clr and ilr
#Zero estimator can not use "rem" because all rows contain a zero 
for (i in c("est","off")) {
  for (j in c("clr", "ilr")) {
    for (k in c("manhattan", "euclidean", "canberra", "gower", "altGower", "horn")){
      ## Using TaxonomyRaw and doing the TSS after normalization this also means normalization according to genome size is not implemented. 
      ## Can use different filtering strategies. 1 Bayseian approach estimating zeroes, 2 offset of 1 and removing all rows containing zeroes.          Only minor effects observed 
      # f <- codaSeq.filter(ak_op, min.reads=1000, min.prop=0.005, min.occurrence=0.2, samples.by.row=FALSE)
      # filtering of the Counttable depending on rowSums. 
       clrdata <- Testdata[rowSums(Testdata)>0,] #Removing all rows that only contains zeroes
      #For validation make a sample matrix and apply the methods 
      #clrdata <- data.frame(S1=c(1,2,0,2,0,9,0), S2=c(2,4,6,4,12,18,20), S3=c(2,4,0,4,0,18,0), S4=c(2,4,6,4,0,18,20))
      # Specify method to use
      if (i=="est") {
      # 1. Replace 0 values with an estimate. Be 
        clrdata2 <- t(cmultRepl(t(clrdata), method="CZM", label=0))
      } else if (i=="off") {
      # 2. Offset of 1 if using TSS should find lowest value and maybe divide that with 10 or other arbitrary offset
        clrdata2 <- clrdata+1
      } else if (i=="rem") {
      # 3. Remove all rows that contain a zero
        row_sub = apply(clrdata, 1, function(row) all(row !=0 ))
        clrdata2<-clrdata[row_sub,]
      } else {
        print("something is wrong")
      }
      
      ## Maks TSS
      #clrdata2<-sweep(clrdata2, 2, colSums(clrdata2), FUN="/")

      if (j=="clr") {
        clrdata2<-clr(t(clrdata2))
      } else if (j=="ilr") {
        clrdata2<-ilr(t(clrdata2))
      } else {
        print("something is wrong")
      }

      distmatrix<-vegdist(clrdata2, method=k)
      #distmatrix<-data.frame(as.matrix(vegdist(clrdata2, method=k))) #Could also have run it with the dist function distmatrix<-data.frame(as.matrix(dist(clrdata2, method="euclidean"))), but vegdist provides additional dissimilarity indices, but get the same when using the same index. The indices working in real space that is one of the advantages doing the the CODA framework and is therefore recommended.  

      #Create list containing the dist matrices
      distName <- paste( 'dist', i, 'total', j, k, sep = '' )
      DistList[[ distName ]]<-distmatrix
    
      #Make distances into matrix
      distmatrix <- data.frame(as.matrix(distmatrix))
      deco<-paste(i, 'total', j, sep='')
      dissi<-k
      df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
    }
  }
}

##Performing zero estimation after total sum scaling, have some implications in that adding a pseudocount of 1 is not very applicable instead it is estimated as min(apply(Testdata, 1, FUN = function(x) {min(x[x > 0])}))/10. The division with 10 to the lowest number is arbitrary and when working with real data more problematic to set since zeros can be structural or due to inadequate sampling. Performing TSS first means knowledge on sensitivity is lost. 
## Maks TSS
clrdata<-sweep(Testdata, 2, colSums(Testdata), FUN="/")
#clr and ilr
#Zero estimator can not use "rem" because all rows contain a zero 
for (i in c("est","off")) {
  for (j in c("clr", "ilr")) {
    for (k in c("manhattan", "euclidean", "canberra", "gower", "altGower", "horn")){
      ## Using TaxonomyRaw and doing the TSS after normalization this also means normalization according to genome size is not implemented. 
      ## Can use different filtering strategies. 1 Bayseian approach estimating zeroes, 2 offset of 1 and removing all rows containing zeroes.    Only minor effects observed 
      # f <- codaSeq.filter(ak_op, min.reads=1000, min.prop=0.005, min.occurrence=0.2, samples.by.row=FALSE)
      # filtering of the Counttable depending on rowSums. 
      clrdata <- clrdata[rowSums(clrdata)>0,] #Removing all rows that only contains zeroes
      #FOr validation make a sample matrix and apply the methods 
      #clrdata <- data.frame(S1=c(1,2,0,2,0,9,0), S2=c(2,4,6,4,12,18,20), S3=c(2,4,0,4,0,18,0), S4=c(2,4,6,4,0,18,20))
      # Specify method to use
      if (i=="est") {
        # 1. Replace 0 values with an estimate. Be 
        clrdata2 <- t(cmultRepl(t(clrdata), method="CZM", label=0))
        } else if (i=="off") {
        # 2. Offset of 1 if using TSS should find lowest value and maybe divide that with 10 or other arbitrary offset
        add<-min(apply(clrdata, 1, FUN = function(x) {min(x[x > 0])}))/10
        clrdata2 <- clrdata+add
        } else if (i=="rem") {
        # 3. Remove all rows that contain a zero
        row_sub = apply(clrdata, 1, function(row) all(row !=0 ))
        clrdata2<-clrdata[row_sub,]
        } else {
        print("something is wrong")
      }

      if (j=="clr") {
        clrdata2<-clr(t(clrdata2))
      } else if (j=="ilr") {
        clrdata2<-ilr(t(clrdata2))
      } else {
        print("something is wrong")
      }

      distmatrix<-vegdist(clrdata2, method=k) #Could also have run it with the dist function distmatrix<-data.frame(as.matrix(dist(clrdata2, method="euclidean"))), but vegdist provides additional dissimilarity indices, but get the same when using the same index. The indices working in real space that is one of the advantages doing the the CODA framework and is therefore recommended.  

      #Create list containing the dist matrices
      distName <- paste( 'dist', 'total', i, j, k, sep = '' )
      DistList[[ distName ]]<-distmatrix
    
      #Make distances into matrix
      distmatrix <- data.frame(as.matrix(distmatrix))
      deco<-paste('total', i, j, sep='')
      dissi<-k
      df<-rbind(df, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
    }
  }
}




###############################Create rank table##################################
dfrank<-bind_cols(df[,1:2], data.frame(t(apply(df[,3:length(df)], 1, rank, ties.method="min"))))

#Save the resuslts as a csv file
#write.table(df, file="StandNormDistAssessmentTable.txt", sep="\t", dec=",", row.names = F, quote = F)

rm(clrdata, clrdata2, distmatrix, deco, dissi, distName, i, j, zero)


#########################################################################
#########################################################################
##Same as above with samples generated as exact variations of reference##
#########################################################################
#########################################################################



#Extracting only the samples that have exact vaues compared to the ref
cols<-c("Ex", "S1Ref")
Testdata<-Testdata[,grepl(paste(cols, collapse="|"), colnames(Testdata))]
Metadata<-Metadata[Metadata$Sample %in% colnames(Testdata), ]
colnames(Testdata)<-sub("S1|S[0-9]Ex|S[0-9][0-9]Ex", "", colnames(Testdata))
Metadata$Sample<-colnames(Testdata)
#write.table(Testdata, file="Testdata.txt", sep="\t")

df2<-data.frame()
DistList2 <- list()
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  for (j in c("total", "max", "freq", "normalize", "pa", "hellinger", "log")) {
    distmatrix<-vegdist(decostand(decostand(t(Testdata), method="total"), method=j), method=i)
    #Create list containing the dist matrices
    distName <- paste( 'dist', i, j, sep = '' )
    DistList2[[ distName ]]<-distmatrix
    #Make distances into matrix
    distmatrix <- data.frame(as.matrix(distmatrix))
    deco<-j
    dissi<-i
    df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
  }
}


#If TSS is performed after other transformations/standardizations/normalizations
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  for (j in c("total", "max", "freq", "normalize", "pa", "hellinger", "log")) {
    distmatrix<-vegdist(decostand(decostand(t(Testdata), method=j), method="total"), method=i)
    #Create list containing the dist matrices
    distName <- paste( 'dist', i, "total", j, sep = '' )
    DistList2[[ distName ]]<-distmatrix
    #Make distances into matrix
    distmatrix <- data.frame(as.matrix(distmatrix))
    deco<-paste(j, "TSS", sep="")
    dissi<-i
    df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
  }
}

##Rarefying using the rrarefy function from vegan
dataRar <- t(rrarefy(t(Testdata), min(colSums(Testdata))))
j <- "Rar"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataRar), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList2[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}


##CSS method from metagenomeseq. The setup to run metagenomeseq is from MixMC
data.metagenomeSeq <- newMRexperiment(Testdata, featureData=NULL, libSize=NULL, normFactors=NULL)  
p <- cumNormStat(data.metagenomeSeq) #default is 0.5
data.cumnorm <- cumNorm(data.metagenomeSeq, p=p)
#CSS data
dataCSS <- MRcounts(data.cumnorm, norm=TRUE, log=TRUE) #A log transformation is also performed as recommended in MixMC 
j <- "CSS"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataCSS), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList2[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}

## Methods from DESeq2
#Create design formula, not important which column is used in metadata since no DA statistics are obtained, but only used to extract size factors.
#The type="ratio" can not be used since all features have at least one zero instead using the "poscounts". 
design <- formula(paste("~ ", "Level"))
#Create DESeq2 object with matrix and metadata
dds <- DESeqDataSetFromMatrix(countData = Testdata,
                              colData = Metadata,
                              design = design)
dds <- estimateSizeFactors(dds, type="poscounts")
dataDESeq<-sweep(Testdata, MARGIN = 2, sizeFactors(dds), FUN = "/")
j <- "DESeq"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataDESeq), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList2[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}

##TMM method from edgeR. Both normalizing according to y$samples$lib.size and y$samples$norm.factors 
#DGEList, not important which column is used in metadata since no DA statistics are obtained.
y <- DGEList(counts=Testdata,group=Metadata$Type)
y <- calcNormFactors(y, method="TMM")
dataTMM<-cpm(y)
j <- "TMM"
for (i in c("manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard", "gower", "altGower", "horn", "binomial")) {
  distmatrix<-vegdist(t(dataTMM), method=i)
  #Create list containing the dist matrices
  distName <- paste( 'dist', i, j, sep = '' )
  DistList2[[ distName ]]<-distmatrix
  #Make distances into matrix
  distmatrix <- data.frame(as.matrix(distmatrix))
  deco<-j
  dissi<-i
  df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
}


###############################Coda framework##################################
##Performing zero estimation before total sum scaling, have some implications in that library size with exact multiplication suddenly is not the same after TSS. 
#clr and ilr
#Zero estimator can not use "rem" because all rows contain a zero 

for (i in c("est","off")) {
  for (j in c("clr", "ilr")) {
    for (k in c("manhattan", "euclidean", "canberra", "gower", "altGower", "horn")){
      ## Using TaxonomyRaw and doing the TSS after normalization this also means normalization according to genome size is not implemented. 
      ## Can use different filtering strategies. 1 Bayseian approach estimating zeroes, 2 offset of 1 and removing all rows containing zeroes.          Only minor effects observed 
      # f <- codaSeq.filter(ak_op, min.reads=1000, min.prop=0.005, min.occurrence=0.2, samples.by.row=FALSE)
      # filtering of the Counttable depending on rowSums. 
       clrdata <- Testdata[rowSums(Testdata)>0,] #Removing all rows that only contains zeroes
      #For validation make a sample matrix and apply the methods 
      #clrdata <- data.frame(S1=c(1,2,0,2,0,9,0), S2=c(2,4,6,4,12,18,20), S3=c(2,4,0,4,0,18,0), S4=c(2,4,6,4,0,18,20))
      # Specify method to use
      if (i=="est") {
      # 1. Replace 0 values with an estimate. Be 
        clrdata2 <- t(cmultRepl(t(clrdata), method="CZM", label=0))
      } else if (i=="off") {
      # 2. Offset of 1 if using TSS should find lowest value and maybe divide that with 10 or other arbitrary offset
        clrdata2 <- clrdata+1
      } else if (i=="rem") {
      # 3. Remove all rows that contain a zero
        row_sub = apply(clrdata, 1, function(row) all(row !=0 ))
        clrdata2<-clrdata[row_sub,]
      } else {
        print("something is wrong")
      }
      
      ## Maks TSS
      #clrdata2<-sweep(clrdata2, 2, colSums(clrdata2), FUN="/")

      if (j=="clr") {
        clrdata2<-clr(t(clrdata2))
      } else if (j=="ilr") {
        clrdata2<-ilr(t(clrdata2))
      } else {
        print("something is wrong")
      }

      distmatrix<-vegdist(clrdata2, method=k) #Could also have run it with the dist function distmatrix<-data.frame(as.matrix(dist(clrdata2, method="euclidean"))), but vegdist provides additional dissimilarity indices, but get the same when using the same index. The indices working in real space that is one of the advantages doing the the CODA framework and is therefore recommended.   

      #Create list containing the dist matrices
      distName <- paste( 'dist', i, 'total', j, k, sep = '' )
      DistList2[[ distName ]]<-distmatrix
    
      #Make distances into matrix
      distmatrix <- data.frame(as.matrix(distmatrix))
      deco<-paste(i, 'total', j, sep='')
      dissi<-k
      df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
    }
  }
}

##Performing zero estimation after total sum scaling, have some implications in that adding a pseudocount of 1 is not very applicable instead it is estimated as min(apply(Testdata, 1, FUN = function(x) {min(x[x > 0])}))/10. The division with 10 to the lowest number is arbitrary and when working with real data more problematic to set since zeros can be structural or due to inadequate sampling. Performing TSS first means knowledge on sensitivity is lost. 
## Maks TSS
clrdata<-sweep(Testdata, 2, colSums(Testdata), FUN="/")
#clr and ilr
#Zero estimator can not use "rem" because all rows contain a zero 
for (i in c("est","off")) {
  for (j in c("clr", "ilr")) {
    for (k in c("manhattan", "euclidean", "canberra", "gower", "altGower", "horn")){
      ## Using TaxonomyRaw and doing the TSS after normalization this also means normalization according to genome size is not implemented. 
      ## Can use different filtering strategies. 1 Bayseian approach estimating zeroes, 2 offset of 1 and removing all rows containing zeroes.    Only minor effects observed 
      # f <- codaSeq.filter(ak_op, min.reads=1000, min.prop=0.005, min.occurrence=0.2, samples.by.row=FALSE)
      # filtering of the Counttable depending on rowSums. 
      clrdata <- clrdata[rowSums(clrdata)>0,] #Removing all rows that only contains zeroes
      #FOr validation make a sample matrix and apply the methods 
      #clrdata <- data.frame(S1=c(1,2,0,2,0,9,0), S2=c(2,4,6,4,12,18,20), S3=c(2,4,0,4,0,18,0), S4=c(2,4,6,4,0,18,20))
      # Specify method to use
      if (i=="est") {
        # 1. Replace 0 values with an estimate.
        clrdata2 <- t(cmultRepl(t(clrdata), method="CZM", label=0))
        } else if (i=="off") {
        # 2. Offset of 1 if using TSS should find lowest value and maybe divide that with 10 or other arbitrary offset
        add<-min(apply(clrdata, 1, FUN = function(x) {min(x[x > 0])}))/10
        clrdata2 <- clrdata+add
        } else if (i=="rem") {
        # 3. Remove all rows that contain a zero
        row_sub = apply(clrdata, 1, function(row) all(row !=0 ))
        clrdata2<-clrdata[row_sub,]
        } else {
        print("something is wrong")
      }

      if (j=="clr") {
        clrdata2<-clr(t(clrdata2))
      } else if (j=="ilr") {
        clrdata2<-ilr(t(clrdata2))
      } else {
        print("something is wrong")
      }

      distmatrix<-vegdist(clrdata2, method=k) #Could also have run it with the dist function distmatrix<-dist(clrdata2, method="euclidean"), but vegdist provides additional dissimilarity indices, but get the same when using the same index. The indices working in real space that is one of the advantages doing the the CODA framework and is therefore recommended. Does not get meaningful results when running with for instance "bray", "jaccard" don't know why  

      #Create list containing the dist matrices
      distName <- paste( 'dist', 'total', i, j, k, sep = '' )
      DistList2[[ distName ]]<-distmatrix
    
      #Make distances into matrix
      distmatrix <- data.frame(as.matrix(distmatrix))
      deco<-paste('total', i, j, sep='')
      dissi<-k
      df2<-rbind(df2, data.frame(deco, dissi, data.frame(distmatrix[1,2:length(distmatrix)])))
    }
  }
}





###############################Create rank table##################################
df2rank<-bind_cols(df2[,1:2], data.frame(t(apply(df2[,3:length(df2)], 1, rank, ties.method="min"))))

#Save the resuslts as a csv file
#write.table(df2, file="StandNormDistAssessmentTable.txt", sep="\t", dec=",", row.names = F, quote = F)

rm(clrdata, clrdata2, distmatrix, deco, dissi, distName, i, j, zero)
```


### Table relative to ref
```{r}
##Show only selected pipelines
#kable(df)
#kable(dfrank)
#kable(df2)
#kable(df2rank)
```


## Visualizations 
### Heatmaps
```{r}
#Create method column
df2$Method <- paste(df2$deco, df2$dissi, sep="") 
row.names(df2)<-df2$Method
df2Heat<-select(df2, -one_of(c("deco", "dissi", "Method")))

##Had to remove method(s) creating NAs
df2Heat <- df2Heat[complete.cases(df2Heat),]
##Then I standardized the orgs into zero mean and unit variance
df2Heat <- data.frame(t(decostand(t(df2Heat), method="max"))) #Can also use scale in pheatmap, but not exactly sure what scaling that is being performed. "max" is used to get comparable indices with the highest dissimilarity being max 
#rowMeans(df2Heat) #COntrol of standardize worked

#plot<-pheatmap(TaxHeatmap, 
#         margins=c(8,8), 
#         treeheight_row = 100, 
#         treeheight_col = 100, 
#         scale="none", 
#         clustering_distance_cols = distmatrix_Species, 
#         clustering_distance_rows = OrgCluster, 
#         annotation_col = colannodf, 
#         cutree_cols = 2, 
#         show_colnames = FALSE, 
#         cellwidth=5, 
#         cellheight=4, 
#         fontsize=6,
#         annotation_colors = annotation_colorsNew[1:5],
#         annotation_legend = TRUE)
#plot_list[[Subset]] = plot[[4]]

#Select which methods to include in heatmap
#rownames(df2Heat)
rows<-c("totalmanhattan", "pamanhattan", "hellingermanhattan", "logmanhattan", "totaleuclidean", "paeuclidean", "hellingereuclidean", "logeuclidean", "totalbray", "pabray", "hellingerbray", "logbray", "hellingerTSSmanhattan", "logTSSmanhattan", "hellingerTSSeuclidean", "logTSSeuclidean", "hellingerTSSbray", "logTSSbray", "Rarmanhattan", "Rareuclidean", "Rarbray", "CSSmanhattan", "CSSbray", "CSSeuclidean", "DESeqmanhattan", "DESeqbray", "DESeqeuclidean", "TMMmanhattan", "TMMbray", "TMMeuclidean", "esttotalclreuclidean", "offtotalclreuclidean", "esttotalilreuclidean", "offtotalilreuclidean", "totaloffclreuclidean", "totaloffilreuclidean")
df2Heat2<-df2Heat[grepl(paste(rows, collapse="|"), rownames(df2Heat)),]

#Make dataframe with Metadata for heatmap annotation
colannodf <- data.frame(Sample=row.names(df2Heat2))
#Create metadata for procrustessumofsquares
colannodf$Preprocessing <-
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*clreuc*", colannodf$Sample), "clr",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*ilreuc*", colannodf$Sample), "ilr",        
          ifelse(seq(along=(colannodf$Sample)) %in% grep("total*", colannodf$Sample), "TSS",
          #ifelse(seq(along=(colannodf$Sample)) %in% grep("max*", colannodf$Sample), "max",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("Rar*", colannodf$Sample), "Rar",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("CSS*", colannodf$Sample), "CSS",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("DESeq*", colannodf$Sample), "DESeq",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("TMM*", colannodf$Sample), "TMM",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("freq*", colannodf$Sample), "freq",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("normalize*", colannodf$Sample), "norm",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("pa*", colannodf$Sample), "pa",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("hellingerTSS*", colannodf$Sample), "hellingerTSS",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("hellinger*", colannodf$Sample), "hellinger",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("logTSS*", colannodf$Sample), "logTSS",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("log*", colannodf$Sample), "log",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("chi.square*", colannodf$Sample), "chisq",
            "Other")))))))))))))))#)

colannodf$betadiversity <-
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*manhattan", colannodf$Sample), "manhattan",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*euclidean", colannodf$Sample), "euclidean",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*canberra", colannodf$Sample), "canberra",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*bray", colannodf$Sample), "bray",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*kulczynski", colannodf$Sample), "kulczynski",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*jaccard", colannodf$Sample), "jaccard",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*gower", colannodf$Sample), "gower",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*altGower", colannodf$Sample), "altGower",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*horn", colannodf$Sample), "horn",
          ifelse(seq(along=(colannodf$Sample)) %in% grep("*binomial", colannodf$Sample), "binomial",
                 "Other"))))))))))

colannodf<-data.frame(colannodf[,2:3], row.names=colannodf[,1])

#Draw the heatmap
plotheatmap<-pheatmap(t(df2Heat2),
          color = colorRampPalette(rev(brewer.pal(n = 7, name = "Blues")))(100),
          scale="none", 
          cellwidth=10, 
          cellheight=8, 
          annotation_col = colannodf,
          treeheight_row = 100, 
          treeheight_col = 100,
          annotation_colors = annotation_colorsHeat,
          annotation_legend = TRUE,
          labels_col = str_replace(rownames(df2Heat2), "bray|manhattan|euclidean", "") %>% str_replace("est", "est_") %>% str_replace("off", "off_") %>% str_replace("clr", "_clr") %>% str_replace("ilr", "_ilr") %>% str_replace("TSS", "_TSS") %>% str_replace("total", "TSS")%>% str_replace("TSSoff__clr", "TSS_off_clr") %>% str_replace("TSSoff__ilr", "TSS_off_ilr"))
        

str_replace(rownames(df2Heat2), "bray|manhattan|euclidean", "") %>% str_replace("est", "est_") %>% str_replace("off", "off_") %>% str_replace("clr", "_clr") %>% str_replace("ilr", "_ilr") %>% str_replace("TSS", "_TSS") %>% str_replace("total", "TSS")%>% str_replace("TSSoff__clr", "TSS_off_clr") %>% str_replace("TSSoff__ilr", "TSS_off_ilr")

pdf(paste("heatmapMethods", ".pdf", sep=""), width=9.5, height=6)
grid.arrange(plotheatmap[[4]])
dev.off() 
```



### PCoA/PCA DistList
```{r}
# Create a list to hold the plot objects.
ScreeList <- list()
StressList <- list()
PCoAList <- list()
j<-1 #Can not just use i 

#Can do it both for DistList and DistList2 (DistList2 Only containing exact changes to ref)
for (i in 1:length(DistList2)) { 
#Multi dimensional scaling with capscale 
    i <- DistList2[[j]] 
    PCoAcsObject<-capscale(i~1)
    #PCoAcsObject<-capscale(DistList2[[200]] ~1)
    
    #Creating plot names
    k<-names(DistList2[j])
    j<-j+1

    #Make stressplot
    #Extract ordination distances and merge with observed dissimilarity
    stress<-stressplot(PCoAcsObject)
    df <- melt(as.matrix(stress))
    names(df)<-c("rowOrd", "colOrd", "OrdDist")
    df<-filter(df, OrdDist>0)
    df2 <- melt(as.matrix(i))
    names(df2)<-c("rowObs", "colObs", "ObsDism")
    df2<-filter(df2, ObsDism>0)
    df<-unite(df, mergecol, c(rowOrd, colOrd), remove=FALSE)
    df2<-unite(df2, mergecol, c(rowObs, colObs), remove=FALSE)
    ggstress<-merge(df, df2, by="mergecol")

    #Create plot name
    pltName <- paste( 'stress', k, sep = '' )
    #create stressplot
    StressList[[ pltName ]]<-ggplot(ggstress) + 
      geom_point(aes(ObsDism, OrdDist)) +
      ggtitle(paste("Stressplot", k, sep=" ")) + 
      labs(x = "Observed dissimilarity", y = "Ordination distance") + 
      theme_bw() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12))


    ##Add eig to plot axes. with cmdscale there are negative values not with capscale
    eig <- PCoAcsObject$CA$eig
    # Calculate the variation explained by PCoA1, 2, 3 and 4
    # and use it to generate axis labels
    eig_1_2 <- eig[1:4] / sum(eig) * 100
    eig_1 <- paste("PCoA1", round(eig_1_2[1], digits = 2), "% variance")
    eig_2 <- paste("PCoA2", round(eig_1_2[2], digits = 2), "% variance")
    eig_3 <- paste("PCoA3", round(eig_1_2[3], digits = 2), "% variance")
    eig_4 <- paste("PCoA4", round(eig_1_2[4], digits = 2), "% variance")

    ##Pull out coordinates for plotting from the ca object
    #Structuring to add to Metadata2
    PCoACA<-PCoAcsObject$CA #The ca object contains the actual ordination results: u ((Weighted) orthonormal site        scores), v ((Weighted)       orthonormal species scores) all na in mine, Xbar (The standardized data matrix after previous stages of analysis), and imaginary.u.eig ???. Info http://cc.oulu.fi/~jarioksa/softhelp/vegan/html/cca.object.html   
    PCoA<-as.data.frame(PCoACA$u)
    #Change colnames. Now add dis and trans info to names 
    colnames(PCoA) <- c("MDS1","MDS2", "MDS3")
    #Add row names to df
    PCoA$Sample <- row.names(PCoA)
    #Merge according to Sample
    Metadata2<-merge(Metadata, PCoA, by="Sample")

    #Create plot name
    pltName <- paste( 'PCoA', k, sep = '' )
    #create PCoA
    PCoAList[[ pltName ]]<-ggplot(Metadata2) + 
      #geom_line(aes(x=MDS1, y=MDS2, group=Matching_samples), size=0.1, linetype="dotted") +  
      geom_jitter(aes(MDS1, MDS2, col=Test, shape=Level), width=0.00, height=0.00, alpha=0.8, size=3, stroke=1.5) +
      #geom_point(aes(MDS1, MDS2, color = Sample_LPSX, group = Sample, shape = Temperature), size=5) +
      scale_color_manual(values=TestCol) +  
      scale_shape_manual(values=LevelShape) +
      ggtitle(paste("PCA/PCoA ", k)) + 
      #labs(colour="Temperature (?C)", shape="Processing", x = eig_1, y = eig_2) + 
      theme_bw() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), legend.position="nothing") #+ 
      #scale_y_reverse() #If you want the y scale reversed, to make plots easier to compare
      #scale_x_reverse() #If you want the x scale reversed, to make plots easier to compare
    #ggsave(paste("CapscalePCoAYoungCOMPARE", "Genus", i, OrgFlt, ".pdf", sep=""), height=6, width=12)
    #ggtitle(paste("PCoA ", i, " n orgs > 1% = ", nrow(Tax2))) #Used with filtering

    #Screeplot 
    screeplot<-data.frame(PCoAcsObject$CA$eig)
    colnames(screeplot)<-c("eig")
    screeplot$eig <- screeplot$eig[1:length(screeplot$eig)] / sum(screeplot$eig) * 100
    screeplot<-add_rownames(screeplot, "MDS")
    screeplot$MDS <- factor(screeplot$MDS, levels=c(sprintf("MDS%d", 1:length(screeplot$eig))))

    #Create plot name
    pltName <- paste( 'scree', k, sep = '' )
    #create screeplot
    ScreeList[[ pltName ]]<-ggplot(screeplot, aes(x=MDS, y=eig)) + 
      geom_bar(stat="identity") + 
      labs(x ="MDS", y ="eig (%)") + 
      ggtitle(paste("Screeplot ", k)) +
      theme_bw() +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), axis.text.x=element_blank(), axis.ticks.x=element_blank()) 
    #ggsave(filename=paste("ScreeplotCapscalePCoA", "Genus", i, OrgFlt, ".pdf", sep=""), height=6, width=12)
    
}

############################################################

## Make PCA and PCoA
lay <- rbind(c(1,2,3,4,5),
             c(6,7,8,9,10),
             c(11,12,13,14,15),
             c(16,17,18,19,20),
             c(21,22,23,24,25),
             c(26,27,28,29,30),
             c(31,32,33,34,35),
             c(36,NA,NA,NA,NA))
#Make pdf
pdf(paste("PCoAorPCA", "Pipelines", ".pdf", sep=""), width=24, height=24)
grid.arrange(PCoAList$PCoAdistmanhattantotal, PCoAList$PCoAdistmanhattanpa , PCoAList$PCoAdistmanhattanhellinger, PCoAList$PCoAdistmanhattanlog, PCoAList$PCoAdisteuclideantotal, PCoAList$PCoAdisteuclideanpa, PCoAList$PCoAdisteuclideanhellinger, PCoAList$PCoAdisteuclideanlog, PCoAList$PCoAdistbraytotal, PCoAList$PCoAdistbraypa, PCoAList$PCoAdistbrayhellinger, PCoAList$PCoAdistbraylog, PCoAList$PCoAdistmanhattantotalhellinger, PCoAList$PCoAdistmanhattantotallog, PCoAList$PCoAdisteuclideantotalhellinger, PCoAList$PCoAdisteuclideantotallog, PCoAList$PCoAdistbraytotalhellinger, PCoAList$PCoAdistbraytotallog, PCoAList$PCoAdistmanhattanRar, PCoAList$PCoAdisteuclideanRar, PCoAList$PCoAdistbrayRar, PCoAList$PCoAdistmanhattanCSS, PCoAList$PCoAdistbrayCSS, PCoAList$PCoAdisteuclideanCSS, PCoAList$PCoAdistmanhattanDESeq, PCoAList$PCoAdistbrayDESeq, PCoAList$PCoAdisteuclideanDESeq, PCoAList$PCoAdistmanhattanTMM, PCoAList$PCoAdistbrayTMM, PCoAList$PCoAdisteuclideanTMM, PCoAList$PCoAdistesttotalclreuclidean, PCoAList$PCoAdistofftotalclreuclidean, PCoAList$PCoAdistesttotalilreuclidean, PCoAList$PCoAdistofftotalilreuclidean, PCoAList$PCoAdisttotaloffclreuclidean, PCoAList$PCoAdisttotaloffilreuclidean, layout_matrix = lay)
dev.off()

#Extract legend
legend<-ggplot(Metadata2) + 
      #geom_line(aes(x=MDS1, y=MDS2, group=Matching_samples), size=0.1, linetype="dotted") +  
      geom_jitter(aes(MDS1, MDS2, col=Test, shape=Level), width=0.00, height=0.00, alpha=0.8, size=3, stroke=1.5) +
      #geom_point(aes(MDS1, MDS2, color = Sample_LPSX, group = Sample, shape = Temperature), size=5) +
      scale_color_manual(values=TestCol) +  
      scale_shape_manual(values=LevelShape) +
      ggtitle(paste("PCA/PCoA ", k)) + 
      #labs(colour="Temperature (?C)", shape="Processing", x = eig_1, y = eig_2) + 
      theme_bw() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), legend.position="bottom")
legendplot<-get_legend(legend)
pdf(paste("LegendPCoA", ".pdf", sep=""), width=36, height=12)
grid.arrange(legendplot)
dev.off()
```





### Procrustes analysis visualized as PCoA
Make protest of all pairwise comparisons and sum of squares store in dist object (proCpairAllmethods)
```{r}
#Selected methods for creating pairwise procrustes correlations/sumofsquares/etc.
DistList2Sel<-DistList2[c("distmanhattantotal", "distmanhattanpa" , "distmanhattanhellinger", "distmanhattanlog", "disteuclideantotal", "disteuclideanpa", "disteuclideanhellinger", "disteuclideanlog", "distbraytotal", "distbraypa", "distbrayhellinger", "distbraylog", "distmanhattantotalhellinger", "distmanhattantotallog", "disteuclideantotalhellinger", "disteuclideantotallog", "distbraytotalhellinger", "distbraytotallog", "distmanhattanRar", "disteuclideanRar", "distbrayRar", "distmanhattanCSS", "distbrayCSS", "disteuclideanCSS", "distmanhattanDESeq", "distbrayDESeq", "disteuclideanDESeq", "distmanhattanTMM", "distbrayTMM", "disteuclideanTMM", "distesttotalclreuclidean", "distofftotalclreuclidean", "distesttotalilreuclidean", "distofftotalilreuclidean", "disttotaloffclreuclidean", "disttotaloffilreuclidean")]
#DistList2Sel<-DistList2[c("distmanhattantotal", "distmanhattanpa" , "distmanhattanhellinger", "distmanhattanlog", "disteuclideantotal", "disteuclideanpa", "disteuclideanhellinger", "disteuclideanlog", "distbraytotal", "distbraypa", "distbrayhellinger", "distbraylog", "distmanhattantotalhellinger", "distmanhattantotallog", "disteuclideantotalhellinger", "disteuclideantotallog", "distbraytotalhellinger", "distbraytotallog", "distmanhattanRar", "disteuclideanRar", "distbrayRar", "distbrayCSS", "disteuclideanCSS", "distmanhattanDESeq", "distbrayDESeq", "disteuclideanDESeq", "distmanhattanTMM", "distbrayTMM", "disteuclideanTMM", "distesttotalclreuclidean", "distofftotalclreuclidean", "distesttotalilreuclidean", "disttotaloffclreuclidean")] #To varify I obtained the same results from ilr and clr
#DistList2Sel<-DistList2 #To select all
                          
k<-1
l<-1
#Create dataframe to hold the correlations
proCpairAllmethods<-setNames(data.frame(matrix(ncol=length(DistList2Sel), nrow=length(DistList2Sel))), c(names(DistList2Sel)))
rownames(proCpairAllmethods)<-c(names(DistList2Sel))
proCpairAllmethodsCor<-proCpairAllmethods
proCpairAllmethodsSS<-proCpairAllmethods
#proCpairAllmethodsCorVec<-numeric()
#proCpairAllmethodsSSVec<-numeric()

#Can do it both for DistList and DistList2 (Only containing exact changes to ref) 
#To test can print the 3 lines below and change length(DistList2) to a smaller value
for (i in 1:length(DistList2Sel)) { 
  for (j in 1:length(DistList2Sel)) {  
    j <- DistList2Sel[[k]]
    #print(names(DistList2Sel[k]))
    i <- DistList2Sel[[l]] 
    #print(names(DistList2Sel[l]))
    
    #Make protest
    prot<-protest(capscale(i~1), capscale(j~1))
    #prot
    #summary(prot)
    #plot(prot)
    #plot(prot, kind=2)
    #Correlations
    #print(prot$t0)
    proCpairAllmethods[k,l]<-(1-prot$t0) #Can try with different measuers correlations (prot$t0), sum of squares (prot$ss). PCoA/PCA title have to be changed accordingly below. Changed this to always being correlations, but other metrics could be provided. 
    #proCpairAllmethodsCorVec<-c(proCpairAllmethodsCorVec, prot$t0) #Vector of raw correlations
    #proCpairAllmethodsSSVec<-c(proCpairAllmethodsCorVec, prot$ss) #Vector of raw sum of squares
    proCpairAllmethodsCor[k,l]<-(prot$t0) #Dist raw correlations
    proCpairAllmethodsSS[k,l]<-(prot$ss) #Have made a seperate object to store sum of squares 
    k <- k+1 
  }
  #Reasign 1 to k and iterate l
  print((l/length(DistList2Sel))*100)
  l <- l+1
  k <- 1
}

#See if a df works with capscale or have to change into class dist and create PCoA
proCpairAllmethods<-as.dist(proCpairAllmethods) #I'm getting the right number of observations corresponding to the lower diagonal the as.dist function default is diag=FALSE, upper=FALSE, auto_convert_data_frames=TRUE.
proCpairAllmethodsCor<-as.dist(proCpairAllmethodsCor)
proCpairAllmethodsSS<-as.dist(proCpairAllmethodsSS)

##Save the resuslts as a csv file
#write.table(data.frame(matrix(proCpairAllmethods)), file="ProcrustesComparingMethodsCor.txt", sep="\t", dec=",", row.names = F, quote = F)
#write.table(data.frame(matrix(proCpairAllmethodsSS)), file="ProcrustesComparingMethodsSS.txt", sep="\t", dec=",", row.names = F, quote = F)
```





### Create PCoA of pairwise procrustes correlations and sum of squares. 
Additional validation plots are provided in the form of stress plots, scree plots and density plots
```{r}
#proCpairAllmethods
#proCpairAllmethodsSS
#proCpairAllmethodsCorVec

##Store PCoAs, stessplots and screeplots in the already created lists
#Create capscale object for both correlations and sum of squares
PCoAcsObject<-capscale(proCpairAllmethods~1)
PCoAcsObjectSS<-capscale(proCpairAllmethodsSS~1)
    
#Make stressplot
#Extract ordination distances and merge with observed dissimilarity
#Correlations
stress<-stressplot(PCoAcsObject)
df <- melt(as.matrix(stress))
names(df)<-c("rowOrd", "colOrd", "OrdDist")
df<-filter(df, OrdDist>0)
df2 <- melt(as.matrix(proCpairAllmethods))
names(df2)<-c("rowObs", "colObs", "ObsDism")
df2<-filter(df2, ObsDism>0)
df<-unite(df, mergecol, c(rowOrd, colOrd), remove=FALSE)
df2<-unite(df2, mergecol, c(rowObs, colObs), remove=FALSE)
ggstress<-merge(df, df2, by="mergecol")
#Create plot name correlations
pltName <- paste( 'stress', 'procrustesCor', sep = '' )
#create stressplot correlations
StressList[[ pltName ]]<-ggplot(ggstress) + 
  geom_point(aes(ObsDism, OrdDist), size=1) + #Can change to 0.01 when running all
  ggtitle(paste("Stress plot", sep=" ")) + 
  labs(x = "Observed dissimilarity", y = "Ordination distance") + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12))

#Sum of squares
stressSS<-stressplot(PCoAcsObjectSS)
dfSS <- melt(as.matrix(stressSS))
names(dfSS)<-c("rowOrd", "colOrd", "OrdDist")
dfSS<-filter(dfSS, OrdDist>0)
df2SS <- melt(as.matrix(proCpairAllmethodsSS))
names(df2SS)<-c("rowObs", "colObs", "ObsDism")
df2SS<-filter(df2SS, ObsDism>0)
dfSS<-unite(dfSS, mergecol, c(rowOrd, colOrd), remove=FALSE)
df2SS<-unite(df2SS, mergecol, c(rowObs, colObs), remove=FALSE)
ggstressSS<-merge(dfSS, df2SS, by="mergecol")
#Create plot name sum of squares
pltName <- paste( 'stress', 'procrustesSS', sep = '' )
#create stressplot sum of squares
StressList[[ pltName ]]<-ggplot(ggstressSS) + 
  geom_point(aes(ObsDism, OrdDist), size=1) + #Can change to 0.01 when running all
  ggtitle(paste("Stress plot", sep=" ")) + 
  labs(x = "Observed dissimilarity", y = "Ordination distance") + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12))


##PCoA correlations
##Add eig to plot axes. with cmdscale there are negative values not with capscale
eig <- PCoAcsObject$CA$eig
# Calculate the variation explained by PCoA1, 2, 3 and 4
# and use it to generate axis labels
eig_1_2 <- eig[1:4] / sum(eig) * 100
eig_1 <- paste("PCoA1", round(eig_1_2[1], digits = 2), "% variance")
eig_2 <- paste("PCoA2", round(eig_1_2[2], digits = 2), "% variance")
eig_3 <- paste("PCoA3", round(eig_1_2[3], digits = 2), "% variance")
eig_4 <- paste("PCoA4", round(eig_1_2[4], digits = 2), "% variance")
##Pull out coordinates for plotting from the ca object
#Structuring to add to Metadata2
PCoACA<-PCoAcsObject$CA #The ca object contains the actual ordination results: u ((Weighted) orthonormal site        scores), v ((Weighted)       orthonormal species scores) all na in mine, Xbar (The standardized data matrix after previous stages of analysis), and imaginary.u.eig ???. Info http://cc.oulu.fi/~jarioksa/softhelp/vegan/html/cca.object.html   
PCoA<-as.data.frame(PCoACA$u)
#Change colnames. Now add dis and trans info to names 
colnames(PCoA) <- c("MDS1","MDS2", "MDS3","MDS4")
#Add row names to df
PCoA$Sample <- row.names(PCoA)

#Create metadata 
MetadataProC<-data.frame(Sample=rownames(PCoA))
MetadataProC$Trans <-
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*totallog", MetadataProC$Sample), "TSS_log",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*totalhellinger", MetadataProC$Sample), "TSS_hellinger",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*clr", MetadataProC$Sample), "clr",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*ilr", MetadataProC$Sample), "ilr",        
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*total", MetadataProC$Sample), "TSS",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*max", MetadataProC$Sample), "max",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*freq", MetadataProC$Sample), "freq",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*normalize", MetadataProC$Sample), "norm",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*pa", MetadataProC$Sample), "pa",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*hellinger", MetadataProC$Sample), "hellinger",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*log", MetadataProC$Sample), "log",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*chi.square", MetadataProC$Sample), "chisq",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*CSS", MetadataProC$Sample), "CSS",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*TMM", MetadataProC$Sample), "TMM",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*DESeq", MetadataProC$Sample), "DESeq",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*Rar", MetadataProC$Sample), "Rarefy",
                 "Other"))))))))))))))))

MetadataProC$Dist <-
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*manhattan*", MetadataProC$Sample), "manhattan",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*euclidean*", MetadataProC$Sample), "euclidean",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*canberra*", MetadataProC$Sample), "canberra",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*bray*", MetadataProC$Sample), "bray",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*kulczynski*", MetadataProC$Sample), "kulczynski",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*jaccard*", MetadataProC$Sample), "jaccard",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*gower*", MetadataProC$Sample), "gower",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*altGower*", MetadataProC$Sample), "altGower",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*horn*", MetadataProC$Sample), "horn",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*binomial*", MetadataProC$Sample), "binomial",
          ifelse(seq(along=(MetadataProC$Sample)) %in% grep("*clr", MetadataProC$Sample), "euclidean",
                 "Other")))))))))))

#Merge according to Sample
MetadataProC2<-merge(MetadataProC, PCoA, by="Sample")

#Create plot name
pltName <- paste( 'PCoA', 'procrustesCor', sep = '' )
#create PCoA
PCoAList[[ pltName ]]<-ggplot(MetadataProC2) + 
  #geom_line(aes(x=MDS1, y=MDS2, group=Matching_samples), size=0.1, linetype="dotted") +  
  geom_jitter(aes(MDS1, MDS2, col=Trans, shape=Dist), width=0.00, height=0.00, alpha=0.8, size=3, stroke=1.5) +
  #geom_point(aes(MDS1, MDS2, color = Sample_LPSX, group = Sample, shape = Temperature), size=5) +
  scale_color_manual(values=ProCCol) +  
  scale_shape_manual(values=ProCShape) +
  ggtitle(paste("PCoA ", 'procrustes rotation 1-correlations')) + 
  labs(colour="Preprocessing", shape="betadiversity", x = eig_1, y = eig_2) + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), legend.position="bottom") #+
  #stat_ellipse(data=filter(MetadataProC2[,1:6], grepl("clr|ilr", Trans)), aes(MDS1, MDS2), level=0.80)
  #scale_y_reverse() #If you want the y scale reversed, to make plots easier to compare
  #scale_x_reverse() #If you want the x scale reversed, to make plots easier to compare
#ggsave(paste("CapscalePCoAYoungCOMPARE", "Genus", i, OrgFlt, ".pdf", sep=""), height=6, width=12)
#ggtitle(paste("PCoA ", i, " n orgs > 1% = ", nrow(Tax2))) #Used with filtering

#Making interactive plot with plotly
ggplotly(ggplot(MetadataProC2[,1:7]) + 
  geom_jitter(aes(MDS1, MDS2, col=Trans, shape=Dist, group=Sample), width=0.00, height=0.00, alpha=0.8, size=3, stroke=1.5) +
  scale_color_manual(values=ProCCol) +  
  scale_shape_manual(values=ProCShape) +
  ggtitle(paste("PCoA ", 'procrustes rotation 1-correlations')) + 
  labs(colour="Transformation", shape="Distance", x = eig_1, y = eig_2) + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), legend.position="bottom"))

##PCoA sum of squares overwriting the objects created above for correlations using the alreade created metadata
##Add eig to plot axes. with cmdscale there are negative values not with capscale
eig <- PCoAcsObjectSS$CA$eig
# Calculate the variation explained by PCoA1, 2, 3 and 4
# and use it to generate axis labels
eig_1_2 <- eig[1:4] / sum(eig) * 100
eig_1 <- paste("PCoA1", round(eig_1_2[1], digits = 2), "% variance")
eig_2 <- paste("PCoA2", round(eig_1_2[2], digits = 2), "% variance")
eig_3 <- paste("PCoA3", round(eig_1_2[3], digits = 2), "% variance")
eig_4 <- paste("PCoA4", round(eig_1_2[4], digits = 2), "% variance")
##Pull out coordinates for plotting from the ca object
#Structuring to add to Metadata2
PCoACA<-PCoAcsObjectSS$CA #The ca object contains the actual ordination results: u ((Weighted) orthonormal site        scores), v ((Weighted)       orthonormal species scores) all na in mine, Xbar (The standardized data matrix after previous stages of analysis), and imaginary.u.eig ???. Info http://cc.oulu.fi/~jarioksa/softhelp/vegan/html/cca.object.html   
PCoA<-as.data.frame(PCoACA$u)
#Change colnames. Now add dis and trans info to names 
colnames(PCoA) <- c("MDS1","MDS2", "MDS3","MDS4")
#Add row names to df
PCoA$Sample <- row.names(PCoA)

#Merge according to Sample
MetadataProC2<-merge(MetadataProC, PCoA, by="Sample")

#Create plot name
pltName <- paste( 'PCoA', 'procrustesSS', sep = '' )
#create PCoA
PCoAList[[ pltName ]]<-ggplot(MetadataProC2) + 
  #geom_line(aes(x=MDS1, y=MDS2, group=Matching_samples), size=0.1, linetype="dotted") +  
  geom_jitter(aes(MDS1, MDS2, col=Trans, shape=Dist), width=0.00, height=0.00, alpha=0.8, size=3, stroke=1.5) +
  #geom_point(aes(MDS1, MDS2, color = Sample_LPSX, group = Sample, shape = Temperature), size=5) +
  scale_color_manual(values=ProCCol) +  
  scale_shape_manual(values=ProCShape) +
  ggtitle(paste("PCoA ", 'procrustes rotation sum of squares')) + 
  labs(colour="Transformation", shape="Distance", x = eig_1, y = eig_2) + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), legend.position="bottom") #+ 
  #scale_y_reverse() #If you want the y scale reversed, to make plots easier to compare
  #scale_x_reverse() #If you want the x scale reversed, to make plots easier to compare
#ggsave(paste("CapscalePCoAYoungCOMPARE", "Genus", i, OrgFlt, ".pdf", sep=""), height=6, width=12)
#ggtitle(paste("PCoA ", i, " n orgs > 1% = ", nrow(Tax2))) #Used with filtering



#Screeplot correlations 
screeplot<-data.frame(PCoAcsObject$CA$eig)
colnames(screeplot)<-c("eig")
screeplot$eig <- screeplot$eig[1:length(screeplot$eig)] / sum(screeplot$eig) * 100
screeplot<-add_rownames(screeplot, "MDS")
screeplot$MDS <- factor(screeplot$MDS, levels=c(sprintf("MDS%d", 1:length(screeplot$eig))))
#Create plot name
pltName <- paste( 'scree', 'procrustesCor', sep = '' )
#create screeplot
ScreeList[[ pltName ]]<-ggplot(screeplot, aes(x=MDS, y=eig)) + 
  geom_bar(stat="identity") + 
  labs(x ="MDS", y ="eig (%)") + 
  ggtitle(paste("Scree plot ")) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), axis.text.x=element_blank(), axis.ticks.x=element_blank()) 
#ggsave(filename=paste("ScreeplotCapscalePCoA", "Genus", i, OrgFlt, ".pdf", sep=""), height=6, width=12)


#Screeplot sum of squares 
screeplot<-data.frame(PCoAcsObjectSS$CA$eig)
colnames(screeplot)<-c("eig")
screeplot$eig <- screeplot$eig[1:length(screeplot$eig)] / sum(screeplot$eig) * 100
screeplot<-add_rownames(screeplot, "MDS")
screeplot$MDS <- factor(screeplot$MDS, levels=c(sprintf("MDS%d", 1:length(screeplot$eig))))
#Create plot name
pltName <- paste( 'scree', 'procrustesSS', sep = '' )
#create screeplot
ScreeList[[ pltName ]]<-ggplot(screeplot, aes(x=MDS, y=eig)) + 
  geom_bar(stat="identity") + 
  labs(x ="MDS", y ="eig (%)") + 
  ggtitle(paste("Scree plot ")) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12), axis.text.x=element_blank(), axis.ticks.x=element_blank()) 
#ggsave(filename=paste("ScreeplotCapscalePCoA", "Genus", i, OrgFlt, ".pdf", sep=""), height=6, width=12)


#Create histogram and density plot for correlations and sum of squres. Sensitivity analysis 
#The histogram and density plot is to be used as an assessment of further investigations are needed in order to decide on how to process metagenomics data. 
Correls<-melt_dist(matrix(proCpairAllmethodsCor))
PCoAList$densityplotCor<-ggplot(Correls, aes(x=dist)) + 
  geom_histogram(aes(y=..density..), binwidth=0.01, colour="black", fill="white") + #..density.. is to have it scale with geom_density
  geom_density(alpha=.25, fill="#FF6666") + 
  labs(x ="Correlation", y ="Density") + 
  ggtitle(paste("Density plot ")) +
  xlim(0,1) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12)) 

Sumssqs<-melt_dist(matrix(proCpairAllmethodsSS))
PCoAList$densityplotSS<-ggplot(Sumssqs, aes(x=dist)) + 
  geom_histogram(aes(y=..density..), binwidth=0.01, colour="black", fill="white") + #..density.. is to have it scale with geom_density
  geom_density(alpha=.25, fill="#FF6666") + 
  labs(x ="Sum of squares", y ="Density") + 
  ggtitle(paste("Density plot ")) +
  xlim(0,1) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title=element_text(size=12)) 


###################################################

#Have the plots stored in lists
lay <- rbind(c(1,1),
             c(1,1),
             c(2,3))

## Create figure with correlations
pdf(paste("PCoAprocrustesrotationcorrelations", ".pdf", sep=""), width=12.5, height=10)
grid.arrange(PCoAList$PCoAprocrustesCor, StressList$stressprocrustesCor, ScreeList$screeprocrustesCor, layout_matrix = lay)
dev.off()

## Create figure with sum of squares
pdf(paste("PCoAprocrustesrotationsumofsquares", ".pdf", sep=""), width=12.5, height=10)
grid.arrange(PCoAList$PCoAprocrustesSS, StressList$stressprocrustesSS, ScreeList$screeprocrustesSS, layout_matrix = lay)
dev.off()

#Have the plots stored in lists
lay <- rbind(c(1,1,1,1),
             c(1,1,1,1),
             c(2,2,3,4))

## Create figure with correlations
pdf(paste("PCoAprocrustesrotationcorrelationsDensity", ".pdf", sep=""), width=12.5, height=10)
grid.arrange(PCoAList$PCoAprocrustesCor, PCoAList$densityplotCor, StressList$stressprocrustesCor, ScreeList$screeprocrustesCor, layout_matrix = lay)
dev.off()

## Create figure with sum of squares
pdf(paste("PCoAprocrustesrotationsumofsquaresDensity", ".pdf", sep=""), width=12.5, height=10)
grid.arrange(PCoAList$PCoAprocrustesSS, PCoAList$densityplotSS, StressList$stressprocrustesSS, ScreeList$screeprocrustesSS, layout_matrix = lay)
dev.off()
```


## Additional
### Session information
```{r sesson_info}
sessionInfo()
```

### This document was processed on: 
```{r}
Sys.Date()
```





